# Session 4: Alerting and Performance Monitoring (90 minutes)

## Introduction
In this session, we'll build upon our synthetic monitoring infrastructure by adding alerts. At the end of this session, you'll have a complete alerting system that notifies you when your services experience issues, allowing for proactive monitoring and rapid incident response.

### Why Synthetic Monitoring Alerts Matter
Alerting is the important, without proper alerts:

- **Silent Failures**: Issues can go unnoticed until customers report them
- **Delayed Response**: Late detection leads to longer outages and more impact
- **No Context**: Manual checking provides limited historical data
- **Inconsistent Monitoring**: Different team members may check different things
- **Alert Fatigue**: Too many false positives reduce team responsiveness

With automated synthetic monitoring alerts:
- **Proactive Detection**: Know about issues before customers do
- **Contextual Information**: Rich metadata helps with faster troubleshooting
- **Consistent Standards**: Everyone gets the same alerts based on the same criteria
- **Historical Tracking**: Build a database of incidents and patterns
- **Scalable Monitoring**: Alert on hundreds of services without manual oversight

## Understanding Grafana Alerting Architecture

### Key Components
Grafana's unified alerting system consists of several key components:

1. **Alert Rules**: Define the conditions that trigger alerts
2. **Data Sources**: Where alert rules query data from (Prometheus, Loki, etc.)
3. **Evaluation Groups**: Collections of alert rules that are evaluated together
4. **Contact Points**: Define how and where to send alert notifications
5. **Notification Policies**: Route alerts to appropriate contact points based on labels
6. **Silences**: Temporarily mute alerts during maintenance or known issues

### Alert Rule Anatomy
Each alert rule contains:
- **Query**: The metric query that provides data
- **Condition**: The threshold or condition that triggers the alert
- **Evaluation**: How often to check the condition
- **Annotations**: Human-readable context about the alert
- **Labels**: Metadata used for routing and grouping

## Core Synthetic Monitoring Metrics
Before we create our alerts, let's understand the key metrics that Grafana Synthetics provides:

### Success Rate Metrics
- `probe_all_success_sum`: Total number of successful probes
- `probe_all_success_count`: Total number of probe attempts
- **Usage**: Calculate availability percentage and error rates

### Duration Metrics  
- `probe_all_duration_seconds_sum`: Total time spent on all probes
- `probe_all_duration_seconds_count`: Number of probe duration measurements
- **Usage**: Calculate average response times and latency

### HTTP-Specific Metrics
- `probe_http_status_code`: HTTP response codes
- `probe_http_duration_seconds`: HTTP-specific timing metrics
- **Usage**: Monitor specific HTTP behaviors

### Labels Available
- `instance`: The target being monitored
- `job`: The job name from your synthetic check
- `probe`: The geographic location running the check
- `check_name`: Name of your synthetic check

## Hands-on: Building a Complete Alerting System

In this exercise, we'll create an alerting system for our synthetic monitoring checks. We'll add the top 5 alert rules as described [here](https://grafana.com/blog/2022/01/11/top-5-user-requested-synthetic-monitoring-alerts-in-grafana-cloud/)

### Step 1: Adding the Prometheus Data Source

Before creating alerts, we need to identify our Prometheus data source. In Grafana Cloud, synthetic monitoring metrics are stored in a Prometheus instance.

First, let's create our alerts file structure:

```bash
cd terraform/synthetics
touch alerts.tf
```

Now, let's start by creating a data source reference. Add the following to your `alerts.tf` file:

```terraform
# Grafana Synthetic Monitoring Alerts

# Data source for Synthetic Monitoring metrics
data "grafana_data_source" "prometheus" {
  name = "{your-prometheus-datasource}"
}
```

**Important**: Replace `your-prometheus-datasource` with your actual Prometheus data source name. You can find this in your Grafana Cloud instance under Configuration â†’ Data Sources.

### Step 2: Creating Your First Alert - Target Down Detection

Let's start with the most critical alert: detecting when a target is completely down. This alert will fire when all probes are failing for a target.

Add the following to your `alerts.tf` file:

```terraform
# Step 1: Target Down Alert - fires when a target is down across all probes
resource "grafana_rule_group" "synthetic_monitoring_alerts" {
  name             = "Synthetic Monitoring Alerts"
  folder_uid       = grafana_folder.synthetic_monitoring_alerts.uid
  interval_seconds = 60

  rule {
    name      = "Target Down - All Probes Failing"
    condition = "C"
    
    data {
      ref_id = "A"
      relative_time_range {
        from = 600
        to   = 0
      }
      datasource_uid = data.grafana_data_source.prometheus.uid
      model = jsonencode({
        expr = <<-EOT
          sum by (instance, job) (rate(probe_all_success_sum{}[10m]))
          /
          sum by (instance, job) (rate(probe_all_success_count{}[10m]))
        EOT
        refId      = "A"
        intervalMs = 1000
        maxDataPoints = 43200
      })
    }

    data {
      ref_id = "C"
      relative_time_range {
        from = 0
        to   = 0
      }
      datasource_uid = "__expr__"
      model = jsonencode({
        conditions = [
          {
            evaluator = {
              params = [0]
              type   = "eq"
            }
            operator = {
              type = "and"
            }
            query = {
              model = ""
              params = ["A"]
            }
            reducer = {
              params = []
              type   = "last"
            }
            type = "query"
          }
        ]
        refId = "C"
        type  = "classic_conditions"
      })
    }

    for    = "5m"
    no_data_state   = "NoData"
    exec_err_state  = "Alerting"
    
    annotations = {
      summary     = "Target {{ $labels.instance }} ({{ $labels.job }}) is down - all probes failing"
      description = "The target {{ $labels.instance }} for job {{ $labels.job }} has been unreachable from all probes for more than 5 minutes."
    }
    
    labels = {
      severity = "critical"
      team     = "platform"
    }
  }
}
```

Let's break down what this alert does:

| Component | Purpose | Details |
|-----------|---------|---------|
| `rule_group` | Container for related alert rules | Groups rules that should be evaluated together |
| `name` | Human-readable identifier | Shows up in Grafana UI and notifications |
| `condition` | References the data query that triggers | "C" refers to our condition query |
| `data` block A | Main metric query | Calculates success rate over 10 minutes |
| `data` block C | Condition evaluation | Triggers when success rate equals 0 (completely down) |
| `for` | Sustained duration | Alert only fires after condition is true for 5 minutes |
| `annotations` | Context for humans | Summary and description appear in notifications |
| `labels` | Metadata for routing | Used by notification policies to route alerts |

The PromQL query `sum by (instance, job) (rate(probe_all_success_sum{}[10m])) / sum by (instance, job) (rate(probe_all_success_count{}[10m]))` calculates the success rate by dividing successful probes by total probe attempts over a 10-minute window.

### Step 3: Adding a Reachability Alert

Now let's add an alert for when reachability drops below acceptable levels, but isn't completely down:

```terraform
  # Step 2: Reachability Alert - fires when reachability drops below 90%
  rule {
    name      = "Low Reachability - Below 90%"
    condition = "C"
    
    data {
      ref_id = "A"
      relative_time_range {
        from = 600
        to   = 0
      }
      datasource_uid = data.grafana_data_source.prometheus.uid
      model = jsonencode({
        expr = <<-EOT
          sum by (instance, job) (rate(probe_all_success_sum{}[10m]))
          /
          sum by (instance, job) (rate(probe_all_success_count{}[10m]))
        EOT
        refId      = "A"
        intervalMs = 1000
        maxDataPoints = 43200
      })
    }

    data {
      ref_id = "C"
      relative_time_range {
        from = 0
        to   = 0
      }
      datasource_uid = "__expr__"
      model = jsonencode({
        conditions = [
          {
            evaluator = {
              params = [0.9]
              type   = "lt"
            }
            operator = {
              type = "and"
            }
            query = {
              model = ""
              params = ["A"]
            }
            reducer = {
              params = []
              type   = "last"
            }
            type = "query"
          }
        ]
        refId = "C"
        type  = "classic_conditions"
      })
    }

    for    = "5m"
    no_data_state   = "NoData"
    exec_err_state  = "Alerting"
    
    annotations = {
      summary     = "Low reachability for {{ $labels.instance }} ({{ $labels.job }})"
      description = "Reachability for {{ $labels.instance }} ({{ $labels.job }}) has dropped below 90% (currently {{ $value | humanizePercentage }})."
    }
    
    labels = {
      severity = "warning"
      team     = "platform"
    }
  }
```

This alert uses the same success rate query but triggers when the value is less than 0.9 (90%). Note the `severity = "warning"` label - this is less critical than a complete outage.

### Step 4: Performance Monitoring - High Latency Alert

Performance matters as much as availability. Let's add an alert for high latency:

```terraform
  # Step 3: High Latency Alert - fires when average latency goes above 1 second
  rule {
    name      = "High Latency - Above 1 Second"
    condition = "C"
    
    data {
      ref_id = "A"
      relative_time_range {
        from = 600
        to   = 0
      }
      datasource_uid = data.grafana_data_source.prometheus.uid
      model = jsonencode({
        expr = <<-EOT
          sum by (instance, job) (rate(probe_all_duration_seconds_sum{}[10m]))
          /
          sum by (instance, job) (rate(probe_all_duration_seconds_count{}[10m]))
        EOT
        refId      = "A"
        intervalMs = 1000
        maxDataPoints = 43200
      })
    }

    data {
      ref_id = "C"
      relative_time_range {
        from = 0
        to   = 0
      }
      datasource_uid = "__expr__"
      model = jsonencode({
        conditions = [
          {
            evaluator = {
              params = [1]
              type   = "gt"
            }
            operator = {
              type = "and"
            }
            query = {
              model = ""
              params = ["A"]
            }
            reducer = {
              params = []
              type   = "last"
            }
            type = "query"
          }
        ]
        refId = "C"
        type  = "classic_conditions"
      })
    }

    for    = "5m"
    no_data_state   = "NoData"
    exec_err_state  = "Alerting"
    
    annotations = {
      summary     = "High latency for {{ $labels.instance }} ({{ $labels.job }})"
      description = "Average latency for {{ $labels.instance }} ({{ $labels.job }}) is above 1 second (currently {{ $value }}s)."
    }
    
    labels = {
      severity = "warning"
      team     = "platform"
    }
  }
```

This alert uses duration metrics (`probe_all_duration_seconds_sum` and `probe_all_duration_seconds_count`) to calculate average response time. It triggers when the average exceeds 1 second.

### Step 5: Error Rate Monitoring

Let's add an alert for monitoring overall error rates:

```terraform
  # Step 4: High Error Rate Alert - fires when error rate is above 10%
  rule {
    name      = "High Error Rate - Above 10%"
    condition = "C"
    
    data {
      ref_id = "A"
      relative_time_range {
        from = 600
        to   = 0
      }
      datasource_uid = data.grafana_data_source.prometheus.uid
      model = jsonencode({
        expr = <<-EOT
          1 - (
            sum by (instance, job) (rate(probe_all_success_sum{}[10m]))
            /
            sum by (instance, job) (rate(probe_all_success_count{}[10m]))
          )
        EOT
        refId      = "A"
        intervalMs = 1000
        maxDataPoints = 43200
      })
    }

    data {
      ref_id = "C"
      relative_time_range {
        from = 0
        to   = 0
      }
      datasource_uid = "__expr__"
      model = jsonencode({
        conditions = [
          {
            evaluator = {
              params = [0.1]
              type   = "gt"
            }
            operator = {
              type = "and"
            }
            query = {
              model = ""
              params = ["A"]
            }
            reducer = {
              params = []
              type   = "last"
            }
            type = "query"
          }
        ]
        refId = "C"
        type  = "classic_conditions"
      })
    }

    for    = "5m"
    no_data_state   = "NoData"
    exec_err_state  = "Alerting"
    
    annotations = {
      summary     = "High error rate for {{ $labels.instance }} ({{ $labels.job }})"
      description = "Error rate for {{ $labels.instance }} ({{ $labels.job }}) is above 10% (currently {{ $value | humanizePercentage }})."
    }
    
    labels = {
      severity = "warning"
      team     = "platform"
    }
  }
```

This query calculates error rate by subtracting the success rate from 1 (`1 - success_rate = error_rate`). It triggers when errors exceed 10%.

### Step 6: Geographic-Specific Monitoring

Sometimes a specific geographic location may have issues while others work fine. Let's add probe-specific monitoring:

```terraform
  # Step 5: High Error Rate by Probe Alert - fires when error rate on a specific probe is above 50%
  rule {
    name      = "High Error Rate by Probe - Above 50%"
    condition = "C"
    
    data {
      ref_id = "A"
      relative_time_range {
        from = 600
        to   = 0
      }
      datasource_uid = data.grafana_data_source.prometheus.uid
      model = jsonencode({
        expr = <<-EOT
          1 - (
            sum by (instance, job, probe) (rate(probe_all_success_sum{}[10m]))
            /
            sum by (instance, job, probe) (rate(probe_all_success_count{}[10m]))
          )
        EOT
        refId      = "A"
        intervalMs = 1000
        maxDataPoints = 43200
      })
    }

    data {
      ref_id = "C"
      relative_time_range {
        from = 0
        to   = 0
      }
      datasource_uid = "__expr__"
      model = jsonencode({
        conditions = [
          {
            evaluator = {
              params = [0.5]
              type   = "gt"
            }
            operator = {
              type = "and"
            }
            query = {
              model = ""
              params = ["A"]
            }
            reducer = {
              params = []
              type   = "last"
            }
            type = "query"
          }
        ]
        refId = "C"
        type  = "classic_conditions"
      })
    }

    for    = "5m"
    no_data_state   = "NoData"
    exec_err_state  = "Alerting"
    
    annotations = {
      summary     = "High error rate on probe {{ $labels.probe }} for {{ $labels.instance }} ({{ $labels.job }})"
      description = "Error rate on probe {{ $labels.probe }} for {{ $labels.instance }} ({{ $labels.job }}) is above 50% (currently {{ $value | humanizePercentage }})."
    }
    
    labels = {
      severity = "warning"
      team     = "platform"
      probe    = "{{ $labels.probe }}"
    }
  }
}
```

This alert includes the `probe` dimension in the query (`sum by (instance, job, probe)`), allowing us to detect issues specific to geographic locations.

### Step 7: Creating an Alert Folder

Organization is key for managing alerts at scale. Let's create a dedicated folder:

```terraform
# Step 6: Create a folder for organizing the synthetic monitoring alerts
resource "grafana_folder" "synthetic_monitoring_alerts" {
  title = "Synthetic Monitoring Alerts"
}
```

This folder will contain all our synthetic monitoring alerts, making them easy to find and manage in the Grafana UI.

### Step 8: Setting Up Contact Points

Alerts are useless if nobody receives them. Let's create a contact point for notifications:

```terraform
# Step 7: Contact point for synthetic monitoring alerts
resource "grafana_contact_point" "synthetic_monitoring_alerts" {
  name = "synthetic-monitoring-alerts"

  email {
    addresses = ["your-email@example.com"]
    subject   = "Grafana Synthetic Monitoring Alert"
    message   = <<-EOT
      Alert: {{ .GroupLabels.alertname }}
      
      {{ range .Alerts }}
      Summary: {{ .Annotations.summary }}
      Description: {{ .Annotations.description }}
      Labels: {{ range .Labels.SortedPairs }}{{ .Name }}={{ .Value }} {{ end }}
      {{ end }}
    EOT
  }
}
```

**Important**: Replace `your-email@example.com` with your actual email address.

The message template uses Grafana's template language to include:
- Alert name
- Summary and description from annotations
- All labels for context

### Step 9: Creating Notification Policies

Contact points define *how* to send alerts, but notification policies define *when* and *to whom*. Let's set up routing:

```terraform
# Step 8: Create a notification policy for these alerts
resource "grafana_notification_policy" "synthetic_monitoring" {
  group_by      = ["alertname", "grafana_folder"]
  contact_point = grafana_contact_point.synthetic_monitoring_alerts.name

  group_wait      = "10s"
  group_interval  = "5m"
  repeat_interval = "12h"

  policy {
    matcher {
      label = "team"  
      match = "="
      value = "platform"
    }
    contact_point   = grafana_contact_point.synthetic_monitoring_alerts.name
    group_wait      = "10s"
    group_interval  = "5m"
    repeat_interval = "4h"
  }
}
```

This policy:
- **Groups alerts** by alert name and folder to reduce notification spam
- **Waits 10 seconds** before sending initial notifications (allows multiple alerts to group)
- **Groups similar alerts** for 5 minutes before sending additional notifications
- **Repeats notifications** every 12 hours for unresolved alerts
- **Routes team=platform alerts** to our contact point with more frequent repeats (4 hours)

### Step 10: Deploying Your Alerting System
Before we can deploy our new alerts, we'll need to update the rights of our Service Account. We'll be creating `folders` and `alerts`, so add those roles and click update in Grafana cloud.

You can find this option under `/org/serviceaccounts`, on this page:
1. Look for `synthetic-acces-policy`
2. Add `Alerting Admin` role
3. Add `Folder Admin` role
4. In the select window click `update`.

Now let's deploy our complete alerting system:

```bash
cd terraform/synthetics
```

First, let's validate our configuration:

```bash
terraform validate
```

If validation passes, let's see what changes will be made:

```bash
terraform plan -var-file="../envs/dev/secrets.auto.tfvars"
```

Review the planned changes. You should see something like this:
- 1 data source to be read
- 1 rule group with 5 alert rules
- 1 folder for organization
- 1 contact point for notifications
- 1 notification policy for routing

If everything looks correct, apply the changes by merging our changes to main.

### Step 11: Testing Your Alerts

Once deployed, you can test your alerts in several ways:

#### Method 1: Using Grafana UI
1. Navigate to your Grafana Cloud instance
2. Go to Alerting â†’ Alert Rules
3. Find your "Synthetic Monitoring Alerts" folder
4. Click on any rule to see its current state and evaluation history

#### Method 2: Temporarily Breaking a Check
You can temporarily modify one of your synthetic checks to point to a non-existent endpoint:

```bash
# Backup your current main.tf
cp main.tf main.tf.backup


```
Edit main.tf to change a target URL to something that will fail. For example, change your HTTP check target to "https://this-will-fail.example.com". Deploy the change and wait for alerts to fire, then restore your backup.

## Understanding Alert States and Lifecycle

### Alert States
Grafana alerts can be in several states:

- **Normal**: Condition is false, no alert
- **Pending**: Condition is true but hasn't exceeded the `for` duration
- **Alerting**: Condition has been true for longer than the `for` duration
- **NoData**: Query returned no data
- **Error**: Query execution failed

### Alert Lifecycle
1. **Evaluation**: Grafana runs queries according to the rule group interval
2. **Condition Check**: Results are compared against thresholds
3. **State Transition**: Alert state changes based on results
4. **Notification**: If state is "Alerting", notifications are sent according to policies
5. **Resolution**: When condition becomes false, alert returns to "Normal"

### Best Practices for Alert Configuration

#### Choosing Thresholds
- **Start Conservative**: Begin with loose thresholds and tighten based on data
- **Consider Baselines**: Use historical data to understand normal patterns
- **Separate Critical vs non-criticals**: Use different thresholds for different severities

#### Setting Evaluation Intervals
- **Balance Responsiveness vs Load**: More frequent evaluation catches issues faster but uses more resources
- **Match Your SLAs**: If you have a 5-minute SLA, don't evaluate every 30 seconds
- **Consider Data Granularity**: Don't evaluate faster than your metric collection interval

#### Structuring Notification Policies
- **Route by Severity**: Critical alerts should wake people up; warnings can wait
- **Group Related Alerts**: Avoid spam by grouping related alerts
- **Time-Based Routing**: Different contacts for business hours vs off-hours

## Further Reading and Resources

### Grafana Documentation
- [Grafana Unified Alerting](https://grafana.com/docs/grafana/latest/alerting/)
- [PromQL Documentation](https://prometheus.io/docs/prometheus/latest/querying/basics/)
- [Alert Notification Templates](https://grafana.com/docs/grafana/latest/alerting/manage-notifications/template-notifications/)

### Best Practices
- [Site Reliability Engineering Book](https://sre.google/sre-book/monitoring-distributed-systems/)
- [Synthetic Monitoring Best Practices](https://grafana.com/blog/2022/03/10/best-practices-for-alerting-on-synthetic-monitoring-metrics-in-grafana-cloud/)
- [Alert Fatigue Prevention](https://grafana.com/blog/2024/05/14/grafana-alerting-new-tools-to-resolve-incidents-faster-and-avoid-alert-fatigue/)

## Next Steps

With your alerting system in place, consider:

1. **Creating Dashboards**: Visualize your synthetic monitoring data
2. **Setting Up SLOs**: Define and track Service Level Objectives
3. **Implementing Runbooks**: Document response procedures for each alert
4. **Training Your Team**: Ensure everyone understands the alerts and responses
5. **Regular Reviews**: Periodically review and tune your alerts based on experience
